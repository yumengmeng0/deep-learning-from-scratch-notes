{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 第5章 误差反向传播法\n",
    "\n",
    "高效计算权重参数的梯度的方法——误差反向传播法。\n",
    "\n",
    "要正确理解误差反向传播法，我个人认为有两种方法：\n",
    "一种是基于数学式；\n",
    "另一种是基于**计算图**（computational graph）。\n",
    "\n",
    "## 5.1 计算图\n",
    "\n",
    "计算图将计算过程用图形表示出来。\n",
    "\n",
    "### 5.1.1 用计算图求解\n",
    "\n",
    "**问题1**：太郎在超市买了2个100日元一个的苹果，消费税是10%，请计算支付金额。\n",
    "\n",
    "![图5-1.基于计算图求解的问题1的答案](../images/图5-1.基于计算图求解的问题1的答案.PNG)\n",
    "图5-1.基于计算图求解的问题1的答案\n",
    "\n",
    "![图5-2.基于计算图求解的问题1的答案：“苹果的个数”和“消费税”作为变量标在○外面](../images/图5-2.基于计算图求解的问题1的答案：“苹果的个数”和“消费税”作为变量标在○外面.PNG)\n",
    "图5-2.基于计算图求解的问题1的答案：“苹果的个数”和“消费税”作为变量标在○外面\n",
    "\n",
    "**问题2**：太郎在超市买了2个苹果、3个橘子。其中，苹果每个100日元，\n",
    "橘子每个150日元。消费税是10%，请计算支付金额。\n",
    "\n",
    "![图5-3.基于计算图求解的问题2的答案](../images/图5-3.基于计算图求解的问题2的答案.PNG)\n",
    "图5-3.基于计算图求解的问题2的答案\n",
    "\n",
    "用计算图解题：\n",
    "1. 构建计算图。\n",
    "2. 在计算图上，从左向右进行计算\n",
    "\n",
    "这里的第2歩“从左向右进行计算”是一种正方向上的传播，简称为**正向传播**（forward propagation）。\n",
    "正向传播是从计算图出发点到结束点的传播。\n",
    "也可以考虑反向（从图上看的话，就是从右向左）的传播。实际上，这种传播称为**反向传播**（backward propagation）。\n",
    "\n",
    "### 5.1.2 局部计算\n",
    "\n",
    "![图5-4.买了2个苹果和其他很多东西的例子](../images/图5-4.买了2个苹果和其他很多东西的例子.PNG)\n",
    "图5-4.买了2个苹果和其他很多东西的例子\n",
    "\n",
    "### 5.1.3 为何用计算图解题\n",
    "\n",
    "计算图到底有什么优点呢？\n",
    "1. 局部计算，无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题。\n",
    "2. 保存中间结果，利用计算图可以将中间的计算结果全部保存起来。\n",
    "3. 使用计算图最大的原因是，可以通过反向传播高效计算导数。\n",
    "\n",
    "![图5-5.基于反向传播的导数的传递](../images/图5-5.基于反向传播的导数的传递.PNG)\n",
    "图5-5.基于反向传播的导数的传递\n",
    "\n",
    "## 5.2 链式法则\n",
    "\n",
    "反向传播将局部导数向正方向的反方向（从右到左）传递，一开始可能会让人感到困惑。\n",
    "传递这个局部导数的原理，是基于**链式法则**（chain rule）的。\n",
    "\n",
    "### 5.2.1 计算图的反向传播\n",
    "\n",
    "![图5-6.计算图的反向传播：沿着与正方向相反的方向，乘上局部导数](../images/图5-6.计算图的反向传播：沿着与正方向相反的方向，乘上局部导数.PNG)\n",
    "图5-6.计算图的反向传播：沿着与正方向相反的方向，乘上局部导数\n",
    "\n",
    "反向传播的计算顺序是，将信号 $E$ 乘以节点的局部导数（$\\frac{\\partial y}{\\partial x}$），然后将结果传递给下一个节点。\n",
    "\n",
    "通过这样的计算，可以高效地求出导数的值，这是反向传播的要点。\n",
    "\n",
    "### 5.2.2 什么是链式法则\n",
    "\n",
    "**复合函数**：由多个函数构成的函数。\n",
    "\n",
    "$$\n",
    "    z = t^2 \\\\\n",
    "    t = x + y \\tag{5.1}\n",
    "$$\n",
    "\n",
    "链式法则是关于复合函数的导数的性质：\n",
    "    如果某个函数由复合函数表示，则复合函数的导数可以构成复合函数的各个函数的导数的乘积表示。\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} \\tag{5.2}\n",
    "$$\n",
    "\n",
    "先求式（5.1）中的局部导数（偏导数）。\n",
    "$$\n",
    "    \\frac{\\partial z}{\\partial t} = 2t  \\\\\n",
    "    \\frac{\\partial t}{\\partial x} = 1   \\tag{5.3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} = 2t \\cdot 1 = 2(x + y) \\tag{5.4}\n",
    "$$\n",
    "\n",
    "### 5.2.3　链式法则和计算图\n",
    "\n",
    "![图5-7.式（5.4）的计算图：沿着与正方向相反的方向，乘上局部导数后传递](../images/图5-7.PNG)\n",
    "图5-7.式（5.4）的计算图：沿着与正方向相反的方向，乘上局部导数后传递\n",
    "\n",
    "\n",
    "![](../images/图5-8.PNG)\n",
    "\n",
    "反向传播是基于链式法则的。\n",
    "\n",
    "## 5.3 反向传播\n",
    "\n",
    "### 5.3.1 加法节点的反向传播\n",
    "\n",
    "$ z=x+y $ 的导数可由下式解析性地计算出来：\n",
    "$$ \\frac{\\partial z}{\\partial x} = 1    \\\\\n",
    "    \\frac{\\partial z}{\\partial y} = 1\n",
    "    \\tag{5.5} $$\n",
    "\n",
    "![图5-9](../images/图5-9.PNG)\n",
    "图5-9\n",
    "\n",
    "![图5-10](../images/图5-10.PNG)\n",
    "图5-10\n",
    "\n",
    "![图5-11.加法节点的反向传播的具体例子](../images/图5-11.加法节点的反向传播的具体例子.PNG)\n",
    "图5-11.加法节点的反向传播的具体例子\n",
    "\n",
    "### 5.3.2 乘法节点的反向传播\n",
    "\n",
    "$ z = xy $ 式子的导数用式（5.6）表示：\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial z}{\\partial x} = y    \\\\\n",
    "    \\frac{\\partial z}{\\partial y} = x\n",
    "    \\tag{5.6}\n",
    "$$\n",
    "\n",
    "![](../images/图5-12.PNG)\n",
    "图5-12.乘法的反向传播：左图是正向传播，右图是反向传播\n",
    "\n",
    "![图5-13.乘法节点的反向传播的具体例子](../images/图5-13.乘法节点的反向传播的具体例子.PNG)\n",
    "图5-13.乘法节点的反向传播的具体例子\n",
    "\n",
    "### 5.3.3　苹果的例子\n",
    "\n",
    "![图5-14.购买苹果的反向传播的例子](../images/图5-14.购买苹果的反向传播的例子.PNG)\n",
    "图5-14.购买苹果的反向传播的例子\n",
    "\n",
    "![](../images/图5-15.购买苹果和橘子的反向传播的例子：在方块中填入数字，完成反向传播.PNG)\n",
    "图5-15.购买苹果和橘子的反向传播的例子：在方块中填入数字，完成反向传播"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 简单层的实现\n",
    "\n",
    "### 5.4.1 乘法层的实现\n",
    "\n",
    "[ch05/layer_naive.py](/ch05/layer_naive.py)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MulLayer:\n",
    "    \"\"\"\n",
    "    乘法层实现\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y  # 翻转x和y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.797057Z",
     "end_time": "2023-06-13T08:18:10.030994Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "backward()将从上游传来的导数（dout）乘以正向传播的翻转值，然后传给下游。\n",
    "\n",
    "![图5-16.购买2个苹果](../images/图5-16.购买2个苹果.PNG)\n",
    "图5-16.购买2个苹果\n",
    "\n",
    "[ch05/buy_apple.py](/ch05/buy_apple.py)\n",
    "\n",
    "backward()的参数中需要输入“关于正向传播时的输出变量的导数”。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.2 加法层的实现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    \"\"\"\n",
    "    加法层实现\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.811785Z",
     "end_time": "2023-06-13T08:18:10.055947Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![图5-17.购买2个苹果和3个橘子](../images/图5-17.PNG)\n",
    "图5-17.购买2个苹果和3个橘子\n",
    "\n",
    "[ch05/buy_apple_orange.py](/ch05/buy_apple_orange.py)\n",
    "\n",
    "用Python实现图5-17的计算图的过程如下所示："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # 1\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # 2\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # 3\n",
    "price = mul_tax_layer.forward(all_price, tax)  # 4\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # 4\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # 3\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # 2\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # 1\n",
    "\n",
    "print(price)  # 715\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)  # 110 2.2 3.3 165 650"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.845696Z",
     "end_time": "2023-06-13T08:18:10.057943Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 激活函数的实现\n",
    "\n",
    "激活函数的ReLU层和Sigmoid层\n",
    "\n",
    "### 5.5.1 ReLU层\n",
    "\n",
    "激活函数ReLU（Rectified Linear Unit）由下式（5.7）表示。\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\left\\{\\begin{matrix}\n",
    "    x \\ (x>0) \\\\\n",
    "    0 \\ (x \\leqslant 0)\n",
    "\\end{matrix}\\right.\n",
    "\\tag{5.7}\n",
    "$$\n",
    "\n",
    "通过式（5.7），可以求出y关于x的导数，如式（5.8）所示。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} =\n",
    "\\left\\{\\begin{m}\n",
    "    1 \\ (x>0)   \\\\\n",
    "    0 \\ (x \\leqslant 0)\n",
    "\\end{m}\\right.\n",
    "\\tag{5.8}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "![图5-18.ReLU层的计算图](../images/图5-18.ReLU层的计算图.PNG)\n",
    "图5-18.ReLU层的计算图"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0  # 将 <=0 的元素设置为0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.858660Z",
     "end_time": "2023-06-13T08:18:10.073880Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1. , -0.5],\n       [-2. ,  3. ]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.873621Z",
     "end_time": "2023-06-13T08:18:10.088859Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[False,  True],\n       [ True, False]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (x <= 0)\n",
    "mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.887378Z",
     "end_time": "2023-06-13T08:18:10.089836Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.5.2 Sigmoid层\n",
    "\n",
    "sigmoid函数由式（5.9）表示。\n",
    "\n",
    "$$  y = \\frac{1}{1 + \\exp(-x)} \\tag{5.9} $$\n",
    "\n",
    "![](../images/图5-19.sigmoid层的计算图（仅正向传播）.PNG)\n",
    "图5-19.sigmoid层的计算图（仅正向传播）\n",
    "\n",
    "* 步骤1\n",
    "\n",
    "“/”节点表示 $y = \\frac{1}{x}$，它的导数：\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial y}{\\partial x} = - \\frac{1}{x^2} \\\\\n",
    "               = -y^2   \\tag{5.10}\n",
    "$$\n",
    "\n",
    "![步骤1](../images/步骤1.PNG)\n",
    "\n",
    "* 步骤2\n",
    "\n",
    "”+“节点将上游的值原封不动地传给下游。\n",
    "\n",
    "![步骤2](../images/步骤2.PNG)\n",
    "\n",
    "* 步骤3\n",
    "\n",
    "”exp“节点表示 $y=\\exp(x)$ ，它的导数：\n",
    "\n",
    "$$\n",
    "     \\frac{\\partial y}{\\partial x} = \\exp (x)   \\tag{5.11}\n",
    "$$\n",
    "\n",
    "计算图中，上游的值乘以正向传播时的输出（这个例子中是exp(−x)）后，再传给下游。\n",
    "\n",
    "![步骤3](../images/步骤3.PNG)\n",
    "\n",
    "* 步骤4\n",
    "\n",
    "”x“节点将正向传播时的值翻转后做乘法运算。\n",
    "\n",
    "![图5-20.Sigmoid层的计算图](../images/图5-20.Sigmoid层的计算图.PNG)\n",
    "图5-20.Sigmoid层的计算图\n",
    "\n",
    "![图5-21.Sigmoid层的计算图（简洁版）](../images/图5-21.Sigmoid层的计算图（简洁版）.PNG)\n",
    "图5-21.Sigmoid层的计算图（简洁版）\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial y} y^2 \\exp(-x) = \\frac{\\partial L}{\\partial y} \\frac{1}{{1 + \\exp(-x)}^2} \\exp(-x)      \\\\\n",
    "    = \\frac{\\partial L}{\\partial y} \\frac{1}{1 + \\exp(-x)} \\frac{\\exp(-x)}{1 + \\exp(-x)}     \\\\\n",
    "    = \\frac{\\partial L}{\\partial y} y (1 - y)      \\\\ \\tag{5.12}\n",
    "$$\n",
    "\n",
    "![图5-22.Sigmoid层的计算图：可以根据正向传播的输出y计算反向传播](../images/图5-22.Sigmoid层的计算图：可以根据正向传播的输出y计算反向传播.PNG)\n",
    "图5-22 Sigmoid层的计算图：可以根据正向传播的输出y计算反向传播"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.904361Z",
     "end_time": "2023-06-13T08:18:10.089836Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.6 Affine/Softmax层的实现\n",
    "\n",
    "### 5.6.1 Affine层\n",
    "\n",
    "Affine 仿射"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "((2,), (2, 3), (3,))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(2)  # 输入\n",
    "W = np.random.rand(2, 3)  # 权重\n",
    "B = np.random.rand(3)  # 偏置\n",
    "\n",
    "X.shape, W.shape, B.shape  # ((2,), (2, 3), (3,))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.920291Z",
     "end_time": "2023-06-13T08:18:10.091851Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.49269064, 0.64905752, 1.11747324])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.dot(X, W) + B\n",
    "Y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.937246Z",
     "end_time": "2023-06-13T08:18:10.091851Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”。\n",
    "进行仿射变换的处理实现为“Affine层”。\n",
    "\n",
    "几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。\n",
    "\n",
    "![](../images/图5-24.Affine层的计算图.PNG)\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial \\textbf{X}} = \\frac{\\partial L}{\\partial \\textbf{Y}} \\cdot \\textbf{W}^T\n",
    "    \\\\\n",
    "    \\frac{\\partial L}{\\partial \\textbf{W}} = \\textbf{X}^T \\cdot  \\frac{\\partial L}{\\partial \\textbf{Y}}\n",
    "    \\\\\n",
    "    \\tag{5.13}\n",
    "$$\n",
    "\n",
    "![](../images/图5-25.Affine层的反向传播.PNG)\n",
    "图5-25 Affine层的反向传播\n",
    "\n",
    "![](../images/图5-26.PNG)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.6.2 批版本的Affine层\n",
    "\n",
    "\n",
    "![图5-27.批版本的Affine层的计算图](../images/图5-27.批版本的Affine层的计算图.PNG)\n",
    "图5-27.批版本的Affine层的计算图\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0,  0,  0],\n       [10, 10, 10]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])\n",
    "X_dot_W"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.952207Z",
     "end_time": "2023-06-13T08:18:10.091851Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1,  2,  3],\n       [11, 12, 13]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W + B"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.967166Z",
     "end_time": "2023-06-13T08:18:10.091851Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 2, 3],\n       [4, 5, 6]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "dY"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.983124Z",
     "end_time": "2023-06-13T08:18:10.092829Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([5, 7, 9])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY, axis=0)\n",
    "dB"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:09.999080Z",
     "end_time": "2023-06-13T08:18:10.092829Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:10.016034Z",
     "end_time": "2023-06-13T08:18:10.092829Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.6.3 Softmax-with-Loss层\n",
    "\n",
    "![](../images/图5-28.PNG)\n",
    "\n",
    "![图5-29.Softmax-with-Loss层的计算图](../images/图5-29.Softmax-with-Loss层的计算图.PNG)\n",
    "图5-29.Softmax-with-Loss层的计算图\n",
    "\n",
    "![图5-30.“简易版”的Softmax-with-Loss层的计算图](../images/图5-30.“简易版”的Softmax-with-Loss层的计算图.PNG)\n",
    "图5-30.“简易版”的Softmax-with-Loss层的计算图"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None  # 损失\n",
    "        self.y = None  # softmax的输出\n",
    "        self.t = None  # 监督数据（one-hot vector）\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T08:18:10.031991Z",
     "end_time": "2023-06-13T08:18:10.092829Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.7 误差反向传播法的实现\n",
    "\n",
    "ch05/two_layer_net.py\n",
    "\n",
    "\n",
    "### 5.7.1 神经网络学习的全貌图\n",
    "\n",
    "* 前提\n",
    "神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的\n",
    "过程称为学习。神经网络的学习分为下面4个步骤。\n",
    "\n",
    "* 步骤1（mini-batch）\n",
    "从训练数据中随机选择一部分数据。\n",
    "\n",
    "* 步骤2（计算梯度）\n",
    "计算损失函数关于各个权重参数的梯度。\n",
    "\n",
    "* 步骤3（更新参数）\n",
    "将权重参数沿梯度方向进行微小的更新。\n",
    "\n",
    "* 步骤4（重复）\n",
    "重复步骤1、步骤2、步骤3。\n",
    "\n",
    "### 5.7.2　对应误差反向传播法的神经网络的实现\n",
    "\n",
    "2层神经网络实现为TwoLayerNet：\n",
    "[/ch05/two_layer_net.py](/ch05/two_layer_net.py)\n",
    "\n",
    "### 5.7.3 误差反向传播法的梯度确认\n",
    "\n",
    "两种求梯度的方法：\n",
    "1. 基于数值微分\n",
    "2. 解析性地求解数学式：误差反向传播法\n",
    "\n",
    "在确认误差反向传播法的实现是否正确时，是需要用到数值微分的。\n",
    "\n",
    "确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为**梯度确认**（gradient check）。\n",
    "\n",
    "[梯度确认的代码实现](/ch05/gradient_check.py)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.7.4 使用误差反向传播法学习\n",
    "\n",
    "[/ch05/train_neuralnet.py](/ch05/train_neuralnet.py)\n",
    "\n",
    "## 5.8 小结\n",
    "\n",
    "* 通过使用计算图，可以直观地把握计算过程\n",
    "* 计算图的节点是由局部计算构成的。局部计算构成全局计算。\n",
    "* 计算图的正向传播进行一般的计算。通过计算图的反向传播，可以计算各个节点的导数。\n",
    "* 通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传播法）。\n",
    "* 通过比较数值微分和误差反向传播法的结果，可以确认误差反向传播法的实现是否正确（梯度确认）。"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
